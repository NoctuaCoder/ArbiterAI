# ğŸ¦‰ ArbiterAI - Autonomous Code Agent

<div align="center">

![ArbiterAI Banner](https://img.shields.io/badge/AI-Code_Agent-blueviolet?style=for-the-badge&logo=openai)
![Python](https://img.shields.io/badge/Python-3.11+-blue?style=for-the-badge&logo=python)
![React](https://img.shields.io/badge/React-18-61DAFB?style=for-the-badge&logo=react)
![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?style=for-the-badge&logo=docker)
![Ollama](https://img.shields.io/badge/Ollama-Powered-000000?style=for-the-badge)

**Um agente de cÃ³digo autÃ´nomo que planeja, executa e entrega soluÃ§Ãµes em tempo real.**

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– Docs](#-documentation) â€¢ [ğŸ¯ Features](#-features) â€¢ [ğŸ³ Docker](#-docker-deployment)

</div>

---

## ğŸ¯ O Que Ã‰ ArbiterAI?

**ArbiterAI** Ã© um agente de cÃ³digo inteligente inspirado em assistentes como **Cursor**, **Copilot** e **Antigravity**. Ele usa **LLMs locais** (via Ollama) para:

1. ğŸ§  **Planejar** tarefas de programaÃ§Ã£o em etapas detalhadas
2. âš™ï¸ **Executar** cada etapa com simulaÃ§Ã£o realista
3. ğŸ“¡ **Transmitir** resultados em tempo real via WebSocket
4. ğŸ’¬ **Interagir** atravÃ©s de uma interface de chat moderna

**Diferencial**: 100% local, sem APIs pagas, sem limites de tokens, sem censura.

---

## âœ¨ Features

### Backend (Python + FastAPI)
- ğŸ¦¾ **SimpleAgent**: Classe de agente com integraÃ§Ã£o Ollama
- ğŸŒ **WebSocket Server**: ComunicaÃ§Ã£o em tempo real
- ğŸ³ **Docker Ready**: DetecÃ§Ã£o inteligente de rede (host.docker.internal)
- ğŸ”„ **Auto-Reconnect**: LÃ³gica de reconexÃ£o automÃ¡tica
- ğŸ“Š **Health Checks**: Endpoints de monitoramento

### Frontend (React + TypeScript)
- ğŸ’¬ **Chat Interface**: UI moderna e responsiva
- ğŸ¨ **Tailwind CSS**: Design system premium
- ğŸ”Œ **WebSocket Client**: ConexÃ£o em tempo real
- ğŸ“± **Responsive**: Mobile-first design
- ğŸ­ **Status Tracking**: Idle â†’ Planning â†’ Executing

### Deployment
- ğŸ³ **Docker Compose**: OrquestraÃ§Ã£o simplificada
- ğŸ§ **Linux Compatible**: Testado em Arch, Ubuntu, Debian
- ğŸ **Mac/Windows**: Suporte via Docker Desktop
- ğŸ”§ **Environment Vars**: ConfiguraÃ§Ã£o flexÃ­vel

---

## ğŸš€ Quick Start

### PrÃ©-requisitos

```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Baixar um modelo (escolha um)
ollama pull llama2          # Geral
ollama pull deepseek-coder  # Especializado em cÃ³digo
ollama pull codellama       # Code-focused

# Iniciar Ollama
ollama serve
```

### OpÃ§Ã£o 1: Docker Compose (Recomendado)

```bash
# Clone o repositÃ³rio
git clone https://github.com/NoctuaCoder/ArbiterAI.git
cd ArbiterAI

# Configure o modelo (opcional)
export OLLAMA_MODEL=deepseek-coder

# Inicie o backend
docker-compose up -d

# Inicie o frontend
cd frontend
npm install
npm run dev
```

Acesse: **http://localhost:5173** ğŸ‰

### OpÃ§Ã£o 2: Desenvolvimento Local

```bash
# Backend
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
python websocket_server.py

# Frontend (novo terminal)
cd frontend
npm install
npm run dev
```

---

## ğŸ® Como Usar

1. **Abra** `http://localhost:5173`
2. **Digite** uma tarefa de programaÃ§Ã£o:
   ```
   Create a Python REST API with FastAPI for user management
   ```
3. **Observe** o agente:
   - ğŸ§  Gerar um plano detalhado
   - âš™ï¸ Executar cada etapa
   - ğŸ“Š Mostrar resultados em tempo real
   - âœ… Concluir a tarefa

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      WebSocket      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React Frontend â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  FastAPI Backend â”‚
â”‚   (Port 5173)   â”‚   ws://localhost    â”‚   (Port 8000)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       :8000/ws       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â”‚ HTTP
                                                  â–¼
                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚  Ollama Server   â”‚
                                         â”‚  (Port 11434)    â”‚
                                         â”‚  [DeepSeek/Llama]â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de ExecuÃ§Ã£o

1. **User** â†’ Envia prompt via interface
2. **Frontend** â†’ Transmite via WebSocket
3. **Backend** â†’ Chama `agent.plan(task)`
4. **Ollama** â†’ Gera plano com LLM
5. **Backend** â†’ Executa cada step com `agent.execute_step()`
6. **Frontend** â†’ Recebe e exibe resultados em tempo real

---

## ğŸ³ Docker Deployment

### Para Mac/Windows (Docker Desktop)

```bash
docker-compose up -d
```

O agente **detecta automaticamente** `host.docker.internal`.

### Para Linux

```bash
# OpÃ§Ã£o 1: Gateway IP
docker run -d --name arbiter-backend -p 8000:8000 \
  -e OLLAMA_URL=http://172.17.0.1:11434/api/generate \
  arbiterai-backend

# OpÃ§Ã£o 2: Host Network (mais simples)
docker run -d --name arbiter-backend --network host arbiterai-backend
```

### VerificaÃ§Ã£o

```bash
# Logs do container
docker logs arbiter-backend

# Procure por:
# ğŸ¦‰ SimpleAgent initialized with Ollama URL: http://host.docker.internal:11434/api/generate

# Health check
curl http://localhost:8000/health
# {"status":"healthy"}
```

---

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

```bash
# Backend (.env ou export)
OLLAMA_URL=http://localhost:11434/api/generate  # URL do Ollama
OLLAMA_MODEL=deepseek-coder                     # Modelo a usar

# Frontend (src/components/AgentProvider.tsx)
wsUrl='ws://localhost:8000/ws'  # WebSocket URL
```

### Modelos Recomendados

| Modelo | Tamanho | Uso | Performance |
|--------|---------|-----|-------------|
| `llama2` | 7B | Geral | â­â­â­ |
| `deepseek-coder` | 6.7B | **CÃ³digo** | â­â­â­â­â­ |
| `codellama` | 7B | CÃ³digo | â­â­â­â­ |
| `mistral` | 7B | Geral | â­â­â­â­ |

**RecomendaÃ§Ã£o**: Use `deepseek-coder` para melhor qualidade em tarefas de programaÃ§Ã£o.

---

## ğŸ“ Estrutura do Projeto

```
ArbiterAI/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ agent_framework.py      # ğŸ§  Classe SimpleAgent
â”‚   â”œâ”€â”€ websocket_server.py     # ğŸŒ FastAPI WebSocket
â”‚   â”œâ”€â”€ requirements.txt        # ğŸ“¦ DependÃªncias Python
â”‚   â”œâ”€â”€ Dockerfile             # ğŸ³ Container config
â”‚   â””â”€â”€ .env.example           # âš™ï¸ Exemplo de config
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ AgentProvider.tsx  # ğŸ”Œ WebSocket Context
â”‚   â”‚   â”‚   â””â”€â”€ Home.tsx          # ğŸ’¬ Chat Interface
â”‚   â”‚   â”œâ”€â”€ App.tsx               # ğŸ¯ Main App
â”‚   â”‚   â””â”€â”€ main.tsx              # ğŸš€ Entry Point
â”‚   â”œâ”€â”€ package.json              # ğŸ“¦ DependÃªncias Node
â”‚   â””â”€â”€ vite.config.ts            # âš¡ Vite Config
â”œâ”€â”€ docker-compose.yml            # ğŸ³ OrquestraÃ§Ã£o
â””â”€â”€ README.md                     # ğŸ“– Este arquivo
```

---

## ğŸ› ï¸ Desenvolvimento

### Backend

```bash
cd backend

# Testar agent framework
python agent_framework.py

# Rodar servidor com reload
uvicorn websocket_server:app --reload --port 8000
```

### Frontend

```bash
cd frontend

# Dev server com HMR
npm run dev

# Build para produÃ§Ã£o
npm run build

# Preview do build
npm run preview
```

---

## ğŸ› Troubleshooting

### Backend nÃ£o conecta ao Ollama

```bash
# Verificar se Ollama estÃ¡ rodando
curl http://localhost:11434/api/tags

# Se nÃ£o estiver, inicie
ollama serve

# Verificar logs do container
docker logs arbiter-backend
```

### Frontend nÃ£o conecta ao WebSocket

```bash
# Verificar se backend estÃ¡ rodando
curl http://localhost:8000/health

# Verificar console do browser (F12)
# Procure por erros de WebSocket
```

### Porta jÃ¡ em uso

```bash
# Encontrar processo usando porta 8000
lsof -ti:8000 | xargs kill -9

# Ou use porta diferente
docker run -p 8001:8000 arbiterai-backend
```

---

## ğŸ¯ Roadmap

- [ ] ExecuÃ§Ã£o real de cÃ³digo (sandbox)
- [ ] Suporte a mÃºltiplos modelos simultÃ¢neos
- [ ] HistÃ³rico de conversas persistente
- [ ] Exportar cÃ³digo gerado
- [ ] IntegraÃ§Ã£o com Git
- [ ] Plugins e extensÃµes
- [ ] API REST alÃ©m do WebSocket
- [ ] AutenticaÃ§Ã£o e multi-usuÃ¡rio

---

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se livre para:

1. ğŸ´ Fork o projeto
2. ğŸŒ¿ Criar uma branch (`git checkout -b feature/amazing`)
3. ğŸ’¾ Commit suas mudanÃ§as (`git commit -m 'Add amazing feature'`)
4. ğŸ“¤ Push para a branch (`git push origin feature/amazing`)
5. ğŸ‰ Abrir um Pull Request

---

## ğŸ“„ LicenÃ§a

MIT License - veja [LICENSE](LICENSE) para detalhes.

---

## ğŸ™ Agradecimentos

- **Ollama** - Por tornar LLMs locais acessÃ­veis
- **FastAPI** - Framework web moderno e rÃ¡pido
- **React** - Biblioteca UI poderosa
- **Antigravity** - InspiraÃ§Ã£o para o design do agente

---

<div align="center">

**Feito com ğŸ¦‰ por [NoctuaCoder](https://github.com/NoctuaCoder)**

â­ Se este projeto te ajudou, deixe uma estrela!

[Report Bug](https://github.com/NoctuaCoder/ArbiterAI/issues) â€¢ [Request Feature](https://github.com/NoctuaCoder/ArbiterAI/issues)

</div>
